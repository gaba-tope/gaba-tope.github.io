---
title: "[NHST] 2장. p-value의 특징"
tags: [statistics]
categories: blog
cover: /files/cover/2024-05-23-p-val-2.png
---
Neyman-Pearson test이 $p$-value를 직접 사용하지는 않지만, 
아래의 내용은 Daniël Lakens의 「[Improving Your Statistical Inferences](https://lakens.github.io/statistical_inferences/)」를 바탕으로 여러 문헌을 공부하며 정리한 내용입니다.

- 제가 통계학 전공자가 아니라는 점을 기억하시고, 이 글에 통계학적으로 잘못된 서술이 있을 수 있다는 점을 유념하여 비판적으로 읽어주길 바라요. 오류를 발견하면 언제든 이야기해주세요.
- 글의 내용을 다른 곳에 인용하실 때에는 이 **글의 출처와 함께 원문의 서지 정보를 필히 밝혀**주세요. 

**Null Statistical Hypothesis Testing** 정리하기 (2/4)

[1장. 통계적 가설검정의 개념]({% post_url 2024-05-22-p-val-1 %}) 

[2장. p value의 특징]({% post_url 2024-05-23-p-val-2 %}) **←**

[3장. p-value를 제대로 보고하기]({% post_url 2024-05-28-p-val-3 %})

[4장. p-value와 error rate에 대한 오해와 물음]({% post_url 2024-05-29-p-val-4 %}) 


$p$-value는 몇가지 특징을 가지는데, 여기서는 두 가지를 살펴보도록 하자. 

## 2-1. P-Value Distribution
시뮬레이션을 통해, 동일한 실험을 반복하여 얻은 $p$-value들의 분포를 알 수 있다.

1. $H_0$가 실제로 거짓일 때: 낮은 p가 나오는 빈도가 높다.
	- 100,000번 반복.
	- $H_0:\mu_1 = \mu_2 = 100$, $\sigma_1 = \sigma_2 = 15$, $n_1 = n_2 = 140$ 
	- power$= 0.80$  

```r
# Adapted from Lakens (2024), section 1.4
p <- numeric(100000) # store all simulated *p*-values

# When H_0 FALSE
for (i in 1:100000) { # for each simulated experiment
  x <- rnorm(n = 140, mean = 100, sd = 15) # 
  y <- rnorm(n = 140, mean = 105, sd = 15) # H_0 FALSE
  p[i] <- t.test(x, y, var.equal = TRUE)$p.value # store the *p*-value
}

(sum(p < 0.05) / 100000) # compute power

hist(p, breaks = 20) # plot a histogram
```

<p align="left">
  		<img src="/files/img/P_when_H0_FALSE.svg" width="60%">
	</p>

2. $H_0$가 실제로 참일 때: p값이 나오는 빈도가 모두 동일하다.
	- 100,000번 반복.
	- $H_0:\mu_1 = \mu_2 = 100$, $\sigma_1 = \sigma_2 = 15$, $n_1 = n_2 = 140$ 
	- $\alpha = 0.05$

    <p align="left">
  		<img src="/files/img/P_when_H0_TRUE.svg" width="60%">
	</p>
	
	- 즉, **$H_0$가 실제로 참일 때는 어떤 p값이든 동일한 확률로 나올 수 있다**는 의미이다.
		- $H_0$가 실제로 참일 때 이를 잘못 기각하는 type I error probability는 $\alpha = 0.05$이다.
		- 실제로 위 시뮬레이션의 빨간 점선은 $y=5000$을 나타내는 선으로, $100,000$번 중 $5,000$번 즉 전체의 $0.05\%$에서 $p$가 통계적으로 유의하지 않게 나오는 것을 볼 수 있다. 다시 말해, $p<0.05$인 경우의 수는 전체의 $0.05\%$ 이하이다.
		- Null hypothesis가 참일 때 $p$-value는 uniform distribution을 따른다.
		- 단, 이러한 분포는 Pearson's $\chi^2$ test나 Fisher's exact test와 같이 test statistic이 discrete한 경우에는 적용되지 않는다 (Wang et al., 2019; Murdoch et al., 2008). 

## 2-2. Lindley's Paradox 

**Lindley's Paradox (or Jeffreys's paradox)**: 빈도주의 관점에서 Jeffreys-Lindley paradox는 power가 증가할수록 효과가 *있을 때*보다 *없을 때*  $p < \alpha = 0.05$인 $p$값이 나올 가능성이 높아지는 역설적인 현상을 말한다[^1] (Maier & Lakens, 2022).

- Power가 높을 때, 가령 $n$이 클 때에, 대부분의 $p$-value가 $\alpha$  이하에 집중되며, 작은 $p$-value는 효과가 있을 때 보다도 없을 때 관찰될 가능성이 더 높다.
- $p$-value가 $\alpha$뿐만 아니라 test의 **power**와도 함께 해석되어야 하는 이유이다.
- 다음은 Lakens (2024) section 1.4를 참고하여 내가 작성한 간단한 시뮬레이션 결과를 바탕으로 그린 그래프이다:
<p align="left">
  		<img src="/files/img/p_density.svg" width="80%">
	</p>

각각 Power가 $50\%$, $80\%$, $99\%$이도록 한 $H_1$이 참인 경우와 $H_0$가 참인 경우, 총 4개 경우에서 Student $t$-test의 결과로 얻은 $p$-value의 분포가 어떻게 다른지 보고자 하였다. Sample size는 $d = 0.5$와 각각의 power를 만족하도록 정하였다. 각 시행을 $100,000$번 반복하여 test를 $100,000$번 진행한 후 얻은 $p$-value histogram을 `ggplot2` package의 [`geom_density()`](https://ggplot2.tidyverse.org/reference/geom_density.html){:target="_blank"} 함수를 통해 추정한 density로 나타냈다. `geom_density()`는 `stat::density()`를 사용하는데, [`stat::density()`](https://rdrr.io/r/stats/density.html){:target="_blank"}는 기본값으로 Gaussian kernel density estimation을 통해 추정한다. 

먼저 시뮬레이션 결과를 `p_null`, `p_99`, `p_80`, `p_50`에 저장하고, 이를 `p_power`에 tibble 타입으로 저장한다.
```r
# alpha = 0.05, H0 TRUE
p_null <- numeric(100000)
for (i in 1:100000) { # for each simulated experiment
  x <- rnorm(n = 100, mean = 0, sd = 1) # Simulate data
  y <- rnorm(n = 100, mean = 0, sd = 1) # Simulate data
  p_null[i] <- t.test(x, y, var.equal = TRUE)$p.value # store the *p*-value
}
(sum(p_null < 0.05) / 100000) # compute alpha

# d = 0.5, 99% power, H1 TRUE
p_99 <- numeric(100000)
for (i in 1:100000) { # for each simulated experiment
  x <- rnorm(n = 150, mean = 0, sd = 1) # Simulate data
  y <- rnorm(n = 150, mean = 0.5, sd = 1) # Simulate data
  p_99[i] <- t.test(x, y, var.equal = TRUE)$p.value # store the *p*-value
}
(sum(p_99 < 0.05) / 100000) # compute power

# d = 0.5, 80% power, H1 TRUE
p_80 <- numeric(100000)
for (i in 1:100000) { # for each simulated experiment
  x <- rnorm(n = 64, mean = 0, sd = 1) # Simulate data
  y <- rnorm(n = 64, mean = 0.5, sd = 1) # Simulate data
  p_80[i] <- t.test(x, y, var.equal = TRUE)$p.value # store the *p*-value
}
(sum(p_80 < 0.05) / 100000) # compute power

# d = 0.5, 50% power, H1 TRUE
p_50 <- numeric(100000)
for (i in 1:100000) { # for each simulated experiment
  x <- rnorm(n = 32, mean = 0, sd = 1) # Simulate data
  y <- rnorm(n = 32, mean = 0.5, sd = 1) # Simulate data
  p_50[i] <- t.test(x, y, var.equal = TRUE)$p.value # store the *p*-value
}
(sum(p_50 < 0.05) / 100000) # compute power

p_power <- tibble(p_null = p_null, p_50 = p_50, p_80 = p_80,
                  p_99 = p_99, blank = numeric(100000))
```

이후 결과를 `ggplot2` 패키지를 통해 그린다. 

```r
cols <- c(p_99 = "black", p_80 = "purple", p_50 = "skyblue",p_null = "red")
p_density <- ggplot(p_power) +
  geom_density(aes(x = p_99, color = "p_99"), linewidth = 0.7, bw = bw_99) + 
  geom_density(aes(x = p_80, color = "p_80"), linewidth = 0.7, bw = bw_80) +
  geom_density(aes(x = p_50, color = "p_50"), linewidth = 0.7, bw = bw_50) +
  geom_density(aes(x = p_null, color = "p_null"), linewidth = 0.7, bw = bw_null,
               linetype = "dashed") + 
  geom_vline(xintercept = 0.05, color = "black", linetype = "dotted") +
  scale_x_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1),
                     expand = expansion(mult = 0, add = 0)) +
  scale_y_continuous(limits = c(0, 10), expand = expansion(mult = 0, add = 0)) +
  labs(x = "p-value", y = "density", title = "Simulated p-value Distribution",
       subtitle = "d = 0.05, repeated 100,000 times",
       caption = "Density estimated by Gaussian KDE", color = "Legend") +
  scale_color_manual(name = "power", values = cols, 
                      labels = c("50%", "80%", "99%", "Under H0")) +
  theme_classic() +
  theme(plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm"),
        plot.title = element_textbox(family = main_font, size = 25, hjust = 0.5),
        plot.subtitle = element_textbox(family = main_font, size = 15, hjust = 0.5,
                                        margin = margin(b = 10)),
        axis.title = element_textbox(family = main_font, size = 15),
        axis.text = element_textbox(family = main_font, size = 10),
        axis.ticks.length = unit(.2, "cm")
        )
```

이때 `geom_density()`의 `bw` 파라미터는 bandwidth를 의미하는데, bandwidth는 Scott의 방법

$$\text{bandwidth} = h \approx 1.06 \times sn^{-1/5}\text{ where n is sample size and s is sample standard deviation.} $$

으로 정하였다[^2]. 

```r
# Bandwidth parameter in Gaussian KDE: Scott's bandwidth
bw_50 <- 1.06*sd(p_50)*length(p_50)^(-0.2)
bw_80 <- 1.06*sd(p_80)*length(p_80)^(-0.2)
bw_99 <- 1.06*sd(p_99)*length(p_99)^(-0.2)
bw_null <- 1.06*sd(p_null)*length(p_null)^(-0.2)
```

## Reference
- Maier, M., & Lakens, D. (2022). Justify Your Alpha: A Primer on Two Practical Approaches. *Advances in Methods and Practices in Psychological Science, 5*(2), 25152459221080396. [https://doi.org/10.1177/25152459221080396](https://doi.org/10.1177/25152459221080396){:target="_blank"} 
- Murdoch, D. J., Tsai, Y.-L., & Adcock, J. (2008). P-Values are Random Variables. *The American Statistician, 62*(3), 242-245. [https://doi.org/10.1198/000313008X332421](https://doi.org/10.1198/000313008X332421){:target="_blank"}  
- Spanos, A. (2013). Who Should Be Afraid of the Jeffreys-Lindley Paradox? Philosophy of Science, 80(1), 73-93. [https://doi.org/10.1086/668875](https://doi.org/10.1086/668875){:target="_blank"}  
- Wagenmakers, E.-J., & Ly, A. (2023). History and nature of the Jeffreys–Lindley paradox. *Archive for History of Exact Sciences, 77*(1), 25-72. [https://doi.org/10.1007/s00407-022-00298-3](https://doi.org/10.1007/s00407-022-00298-3){:target="_blank"}   
- Wang, B., Zhou, Z., Wang, H., M. Tu, X., & Feng, C. (2019). The p-value and model specification in statistics. *General Psychiatry, 32*(3), e100081. [https://doi.org/10.1136/gpsych-2019-100081](https://doi.org/10.1136/gpsych-2019-100081){:target="_blank"} 

[^1]: Jeffreys-Lindley paradox에 관한 아래 내용은 Spanos (2013)의 서술을 정리한 것이다.<br> 'Large $n$ problem'은 Lindley (1957)에 의해 제시되었다. Mean $\theta \in (-\infty,\infty)$와 알려진 variance $\sigma^2 > 0$를 갖는 NIID(Normal, Independent, Identically Distributed) simple Normal model <br> $$X_k \sim NIID(\theta, \sigma^2), k= 1, 2,\ldots, n, \ldots$$<br>에 대해, large $n$ problem은 다음과 같다:<br>**The large $n$ problem**: Frequentist $\alpha$-significance level test는 어떠한 point null hypothesis (e.g. $H_0:\theta = \theta_0$)라도 기각되도록 하는 충분히 큰 sample size $n$을 항상 가진다는 오류에 취약하다.<br>Lindley는 이 결과가 역설적이라고 보는데, Bayesian 관점과 충돌하기 때문이다. <br>**The Jeffreys-Lindley paradox**: 특정한 prior에 대해서, $n\rightarrow\infty$이면 ($H_0$에 대한 posterior probability) $\rightarrow 1$이다.<br> 즉, $n$이 충분히 클 때 frequentist 접근에서 기각되는 $H_0$가 Bayesian 접근에서 기각되지 않을 수 있는 역설적 상황이 벌어지는 것이다.

[^2]: 사실 이 방법을 사용하는 것이 가장 적절한지는 모르겠다. Distribution이 한눈에 보아도 highly skewed되어 있는데, 여기에 Gaussian KDE를 사용하는 것이 맞는지, 또는 bandwidth를 결정할 때 Scott의 방법을 사용하는 것이 맞는지 잘 모르겠다. 하지만 Gaussian KDE에 Scott의 bandwidth를 사용하였을 때, 눈으로 보기에 density가 histogram의 추세를 잘 따르는 것 같아 사용하였다.